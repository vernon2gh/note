From f7ec593ae66956367f8e9914b17831eb54800d92 Mon Sep 17 00:00:00 2001
From: Vernon Yang <vernon2gm@gmail.com>
Date: Sun, 26 Jan 2025 10:58:42 +0800
Subject: [PATCH] Introduce priority memory pool

This feature is priority memory buffer pool support. Such pools are
mostly used for hight priority thread to allocate memory.

Signed-off-by: Vernon Yang <vernon2gm@gmail.com>
---
 include/linux/memcontrol.h       |  11 ++
 include/linux/mmzone.h           |   1 +
 include/linux/priority_mempool.h |  74 +++++++++
 include/linux/vm_event_item.h    |  10 ++
 mm/Kconfig                       |   5 +
 mm/Makefile                      |   1 +
 mm/memcontrol.c                  |  62 +++++++-
 mm/page_alloc.c                  |   5 +
 mm/priority_mempool.c            | 252 +++++++++++++++++++++++++++++++
 mm/vmscan.c                      |   6 +
 mm/vmstat.c                      |  10 ++
 11 files changed, 434 insertions(+), 3 deletions(-)
 create mode 100644 include/linux/priority_mempool.h
 create mode 100644 mm/priority_mempool.c

diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 6e74b8254d9b..c0b37dcd6cc6 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -220,6 +220,11 @@ struct mem_cgroup {
 	 */
 	bool oom_group;
 
+	/*
+	 * priority memory buffer pool support.
+	 */
+	bool pmempool_enable;
+
 	int swappiness;
 
 	/* memory.events and memory.events.local */
@@ -349,6 +354,7 @@ enum objext_flags {
 #ifdef CONFIG_MEMCG
 
 static inline bool folio_memcg_kmem(struct folio *folio);
+bool current_is_direct_kworker(void);
 
 /*
  * After the initialization objcg->memcg is always pointing at
@@ -1063,6 +1069,11 @@ static inline u64 cgroup_id_from_mm(struct mm_struct *mm)
 
 #define MEM_CGROUP_ID_SHIFT	0
 
+static inline bool current_is_direct_kworker(void)
+{
+	return false;
+}
+
 static inline struct mem_cgroup *folio_memcg(struct folio *folio)
 {
 	return NULL;
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 9540b41894da..36ff5668bdae 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -219,6 +219,7 @@ enum node_stat_item {
 	/* PGDEMOTE_*: pages demoted */
 	PGDEMOTE_KSWAPD,
 	PGDEMOTE_DIRECT,
+	PGDEMOTE_DIRECT_KWORKER,
 	PGDEMOTE_KHUGEPAGED,
 #ifdef CONFIG_HUGETLB_PAGE
 	NR_HUGETLB,
diff --git a/include/linux/priority_mempool.h b/include/linux/priority_mempool.h
new file mode 100644
index 000000000000..58965f0dd9fc
--- /dev/null
+++ b/include/linux/priority_mempool.h
@@ -0,0 +1,74 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * priority memory buffer pool support
+ */
+#ifndef _LINUX_PMEMPOOL_H
+#define _LINUX_PMEMPOOL_H
+
+#include <linux/mm_types.h>
+#include "internal.h"
+
+#ifdef CONFIG_PMEMPOOL
+
+struct page *__priority_mempool_alloc(gfp_t gfp_mask, unsigned int order,
+							bool prealloc);
+void priority_mempool_free(struct page *page, unsigned int order);
+
+static inline struct page *priority_mempool_alloc_noprof(gfp_t gfp_mask,
+							unsigned int order)
+{
+	return __priority_mempool_alloc(gfp_mask, order, false);
+}
+
+static inline struct page *priority_mempool_prealloc_noprof(gfp_t gfp_mask,
+							unsigned int order)
+{
+	return __priority_mempool_alloc(gfp_mask, order, true);
+}
+
+#define priority_mempool_alloc(...)					\
+	alloc_hooks(priority_mempool_alloc_noprof(__VA_ARGS__))
+
+#define priority_mempool_prealloc(...)					\
+	alloc_hooks(priority_mempool_prealloc_noprof(__VA_ARGS__))
+
+#else
+
+static inline struct page *__priority_mempool_alloc(gfp_t gfp_mask,
+						    unsigned int order,
+						    bool prealloc)
+{
+	return NULL;
+}
+
+static inline void priority_mempool_free(struct page *page, unsigned int order)
+{
+}
+
+static inline struct page *priority_mempool_alloc_noprof(gfp_t gfp_mask,
+							unsigned int order)
+{
+	return NULL;
+}
+
+static inline struct page *priority_mempool_prealloc_noprof(gfp_t gfp_mask,
+							unsigned int order)
+{
+	return NULL;
+}
+
+static inline struct page *priority_mempool_alloc(gfp_t gfp_mask,
+						  unsigned int order)
+{
+	return NULL;
+}
+
+static inline struct page *priority_mempool_prealloc(gfp_t gfp_mask,
+						     unsigned int order)
+{
+	return NULL;
+}
+
+#endif
+
+#endif /* _LINUX_PMEMPOOL_H */
diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index f70d0958095c..4de80d8f4e55 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -40,15 +40,25 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		PGREUSE,
 		PGSTEAL_KSWAPD,
 		PGSTEAL_DIRECT,
+		PGSTEAL_DIRECT_KWORKER,
 		PGSTEAL_KHUGEPAGED,
 		PGSCAN_KSWAPD,
 		PGSCAN_DIRECT,
+		PGSCAN_DIRECT_KWORKER,
 		PGSCAN_KHUGEPAGED,
 		PGSCAN_DIRECT_THROTTLE,
 		PGSCAN_ANON,
 		PGSCAN_FILE,
 		PGSTEAL_ANON,
 		PGSTEAL_FILE,
+
+#ifdef CONFIG_PMEMPOOL
+		PGPOOL_SUCCESS,
+		PGPOOL_FAILED,
+		PGPOOL_REFILL_FAILED,
+		PGPOOL_REFILL_WAKEUP,
+#endif
+
 #ifdef CONFIG_NUMA
 		PGSCAN_ZONE_RECLAIM_SUCCESS,
 		PGSCAN_ZONE_RECLAIM_FAILED,
diff --git a/mm/Kconfig b/mm/Kconfig
index 1b501db06417..ff7f47e1dce7 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -1358,6 +1358,11 @@ config PT_RECLAIM
 
 	  Note: now only empty user PTE page table pages will be reclaimed.
 
+config PMEMPOOL
+	bool "priority memory pool"
+	help
+	  This feature is priority memory buffer pool support. Such pools are
+	  mostly used for hight priority thread to allocate memory.
 
 source "mm/damon/Kconfig"
 
diff --git a/mm/Makefile b/mm/Makefile
index 850386a67b3e..4e4e5ff18008 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -147,3 +147,4 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o
 obj-$(CONFIG_EXECMEM) += execmem.o
 obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o
 obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o
+obj-$(CONFIG_PMEMPOOL) += priority_mempool.o
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 46f8b372d212..325e346e1794 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -314,6 +314,7 @@ static const unsigned int memcg_node_stat_items[] = {
 #endif
 	PGDEMOTE_KSWAPD,
 	PGDEMOTE_DIRECT,
+	PGDEMOTE_DIRECT_KWORKER,
 	PGDEMOTE_KHUGEPAGED,
 #ifdef CONFIG_HUGETLB_PAGE
 	NR_HUGETLB,
@@ -430,9 +431,11 @@ static const unsigned int memcg_vm_event_stat[] = {
 	PSWPOUT,
 	PGSCAN_KSWAPD,
 	PGSCAN_DIRECT,
+	PGSCAN_DIRECT_KWORKER,
 	PGSCAN_KHUGEPAGED,
 	PGSTEAL_KSWAPD,
 	PGSTEAL_DIRECT,
+	PGSTEAL_DIRECT_KWORKER,
 	PGSTEAL_KHUGEPAGED,
 	PGFAULT,
 	PGMAJFAULT,
@@ -461,6 +464,11 @@ static const unsigned int memcg_vm_event_stat[] = {
 	NUMA_PTE_UPDATES,
 	NUMA_HINT_FAULTS,
 #endif
+#ifdef CONFIG_PMEMPOOL
+	PGPOOL_SUCCESS,
+	PGPOOL_FAILED,
+	PGPOOL_REFILL_WAKEUP,
+#endif
 };
 
 #define NR_MEMCG_EVENTS ARRAY_SIZE(memcg_vm_event_stat)
@@ -1389,6 +1397,7 @@ static const struct memory_stat memory_stats[] = {
 
 	{ "pgdemote_kswapd",		PGDEMOTE_KSWAPD		},
 	{ "pgdemote_direct",		PGDEMOTE_DIRECT		},
+	{ "pgdemote_direct_kworker",	PGDEMOTE_DIRECT_KWORKER	},
 	{ "pgdemote_khugepaged",	PGDEMOTE_KHUGEPAGED	},
 #ifdef CONFIG_NUMA_BALANCING
 	{ "pgpromote_success",		PGPROMOTE_SUCCESS	},
@@ -1431,6 +1440,7 @@ static int memcg_page_state_output_unit(int item)
 	case WORKINGSET_NODERECLAIM:
 	case PGDEMOTE_KSWAPD:
 	case PGDEMOTE_DIRECT:
+	case PGDEMOTE_DIRECT_KWORKER:
 	case PGDEMOTE_KHUGEPAGED:
 #ifdef CONFIG_NUMA_BALANCING
 	case PGPROMOTE_SUCCESS:
@@ -1503,10 +1513,12 @@ static void memcg_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)
 	seq_buf_printf(s, "pgscan %lu\n",
 		       memcg_events(memcg, PGSCAN_KSWAPD) +
 		       memcg_events(memcg, PGSCAN_DIRECT) +
+		       memcg_events(memcg, PGSCAN_DIRECT_KWORKER) +
 		       memcg_events(memcg, PGSCAN_KHUGEPAGED));
 	seq_buf_printf(s, "pgsteal %lu\n",
 		       memcg_events(memcg, PGSTEAL_KSWAPD) +
 		       memcg_events(memcg, PGSTEAL_DIRECT) +
+		       memcg_events(memcg, PGSTEAL_DIRECT_KWORKER) +
 		       memcg_events(memcg, PGSTEAL_KHUGEPAGED));
 
 	for (i = 0; i < ARRAY_SIZE(memcg_vm_event_stat); i++) {
@@ -1963,6 +1975,11 @@ static void high_work_func(struct work_struct *work)
 	reclaim_high(memcg, MEMCG_CHARGE_BATCH, GFP_KERNEL);
 }
 
+bool current_is_direct_kworker(void)
+{
+	return kthread_func(current) == high_work_func;
+}
+
 /*
  * Clamp the maximum sleep time per allocation batch to 2 seconds. This is
  * enough to still cause a significant slowdown in most cases, while still
@@ -2382,9 +2399,13 @@ int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,
 	 * when it rechecks the overage and simply bail out.
 	 */
 	if (current->memcg_nr_pages_over_high > MEMCG_CHARGE_BATCH &&
-	    !(current->flags & PF_MEMALLOC) &&
-	    gfpflags_allow_blocking(gfp_mask))
-		mem_cgroup_handle_over_high(gfp_mask);
+	    !(current->flags & PF_MEMALLOC)) {
+		if (gfpflags_allow_blocking(gfp_mask))
+			mem_cgroup_handle_over_high(gfp_mask);
+		else
+			schedule_work(&memcg->high_work);
+	}
+
 	return 0;
 }
 
@@ -4288,6 +4309,35 @@ static ssize_t memory_oom_group_write(struct kernfs_open_file *of,
 	return nbytes;
 }
 
+static int memory_pmempool_enable_show(struct seq_file *m, void *v)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_seq(m);
+
+	seq_printf(m, "%d\n", READ_ONCE(memcg->pmempool_enable));
+
+	return 0;
+}
+
+static ssize_t memory_pmempool_enable_write(struct kernfs_open_file *of,
+				     char *buf, size_t nbytes, loff_t off)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
+	bool pmempool_enable;
+	int ret;
+
+	buf = strstrip(buf);
+	if (!buf)
+		return -EINVAL;
+
+	ret = kstrtobool(buf, &pmempool_enable);
+	if (ret)
+		return ret;
+
+	WRITE_ONCE(memcg->pmempool_enable, pmempool_enable);
+
+	return nbytes;
+}
+
 enum {
 	MEMORY_RECLAIM_SWAPPINESS = 0,
 	MEMORY_RECLAIM_NULL,
@@ -4430,6 +4480,12 @@ static struct cftype memory_files[] = {
 		.seq_show = memory_oom_group_show,
 		.write = memory_oom_group_write,
 	},
+	{
+		.name = "pmempool.enable",
+		.flags = CFTYPE_NOT_ON_ROOT | CFTYPE_NS_DELEGATABLE,
+		.seq_show = memory_pmempool_enable_show,
+		.write = memory_pmempool_enable_write,
+	},
 	{
 		.name = "reclaim",
 		.flags = CFTYPE_NS_DELEGATABLE,
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 579789600a3c..9db703d774e8 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -55,6 +55,7 @@
 #include <linux/delayacct.h>
 #include <linux/cacheinfo.h>
 #include <linux/pgalloc_tag.h>
+#include <linux/priority_mempool.h>
 #include <asm/div64.h>
 #include "internal.h"
 #include "shuffle.h"
@@ -4378,6 +4379,10 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	if (current->flags & PF_MEMALLOC)
 		goto nopage;
 
+	page = priority_mempool_prealloc(gfp_mask, order);
+	if (page)
+		goto got_pg;
+
 	/* Try direct reclaim and then allocating */
 	page = __alloc_pages_direct_reclaim(gfp_mask, order, alloc_flags, ac,
 							&did_some_progress);
diff --git a/mm/priority_mempool.c b/mm/priority_mempool.c
new file mode 100644
index 000000000000..84af49a4f12f
--- /dev/null
+++ b/mm/priority_mempool.c
@@ -0,0 +1,252 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *  linux/mm/priority_mempool.c
+ *
+ *  priority memory buffer pool support. Such pools are mostly used
+ *  for hight priority thread to allocate memory.
+ *
+ *  started by Vernon Yang, Copyright (C) 2025
+ */
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/sched/rt.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#include <linux/vmstat.h>
+#include <linux/mempool.h>
+#include <linux/priority_mempool.h>
+
+#define MEMPOOL_MIN_NR			1024
+#define MEMPOOL_MAX_MIGRATE_TYPES	MIGRATE_PCPTYPES
+#define MEMPOOL_MAX_ORDER		(PAGE_ALLOC_COSTLY_ORDER + 1)
+
+static mempool_t *pmempool[MEMPOOL_MAX_MIGRATE_TYPES][MEMPOOL_MAX_ORDER];
+
+#define for_each_pmempool(pool, mtype, order)				\
+	for (mtype = 0; mtype < MEMPOOL_MAX_MIGRATE_TYPES; mtype++)	\
+		for (order = 0, pool = look_for_mempool(mtype, order);	\
+				order < MEMPOOL_MAX_ORDER;		\
+				pool = look_for_mempool(mtype, ++order))
+
+static struct task_struct *kpmempoold;
+static struct wait_queue_head wq;
+static int refill;
+
+static struct dentry *pmempool_debugfs_dir;
+static bool enabled;
+
+static inline gfp_t migratetype_gfp(int migratetype)
+{
+	gfp_t gfp_mask = __GFP_COMP | __GFP_NOMEMALLOC | __GFP_KSWAPD_RECLAIM |
+			__GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO;
+
+	if (migratetype == MIGRATE_MOVABLE)
+		gfp_mask |= __GFP_MOVABLE;
+	else if (migratetype == MIGRATE_RECLAIMABLE)
+		gfp_mask |= __GFP_RECLAIMABLE;
+
+	return gfp_mask;
+}
+
+static inline mempool_t *look_for_mempool(int mtype, int order)
+{
+	if (mtype >= MEMPOOL_MAX_MIGRATE_TYPES ||
+	    order >= MEMPOOL_MAX_ORDER)
+		return NULL;
+
+	return pmempool[mtype][order];
+}
+
+static inline bool priority_mempool_enable(struct task_struct *p)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_task(p);
+
+	if (unlikely(enabled))
+		return true;
+
+	if (!mem_cgroup_disabled())
+		return READ_ONCE(memcg->pmempool_enable);
+
+	return false;
+}
+
+struct page *__priority_mempool_alloc(gfp_t gfp_mask, unsigned int order,
+							bool prealloc)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_task(current);
+	mempool_t *pool;
+	struct page *page;
+
+	if (!priority_mempool_enable(current))
+		return NULL;
+
+	/* skip kpmempoold thread */
+	if (current->pid == kpmempoold->pid)
+		return NULL;
+
+	pool = look_for_mempool(gfp_migratetype(gfp_mask), order);
+	if (pool == NULL)
+		return NULL;
+
+	if (prealloc)
+		page = mempool_alloc_preallocated(pool);
+	else
+		page = mempool_alloc_noprof(pool, gfp_mask);
+
+	if (page) {
+		kmsan_free_page(page, order);
+		set_page_count(page, 0);
+	}
+
+	if (!mempool_is_saturated(pool) && (refill == 0)) {
+		refill = 1;
+		wake_up_interruptible(&wq);
+		count_vm_event(PGPOOL_REFILL_WAKEUP);
+		count_memcg_events(memcg, PGPOOL_REFILL_WAKEUP, 1);
+	}
+
+	count_vm_event(page ? PGPOOL_SUCCESS : PGPOOL_FAILED);
+	count_memcg_events(memcg, page ? PGPOOL_SUCCESS : PGPOOL_FAILED, 1);
+
+	return page;
+}
+EXPORT_SYMBOL(__priority_mempool_alloc);
+
+void priority_mempool_free(struct page *page, unsigned int order)
+{
+	mempool_t *pool;
+
+	if (!priority_mempool_enable(current))
+		return;
+
+	pool = look_for_mempool(get_pageblock_migratetype(page), order);
+	if (pool == NULL)
+		return;
+
+	mempool_free(page, pool);
+}
+EXPORT_SYMBOL(priority_mempool_free);
+
+static int pmempool_refill(void *arg)
+{
+	mempool_t *pool;
+	gfp_t gfp_mask;
+	int mtype;
+	int order;
+	void *element;
+
+	while (!kthread_should_stop()) {
+		if (wait_event_interruptible(wq, refill == 1))
+			continue;
+
+		for_each_pmempool(pool, mtype, order) {
+			while (!mempool_is_saturated(pool)) {
+				gfp_mask = migratetype_gfp(mtype);
+				element = pool->alloc(gfp_mask, (void *)(long)order);
+				if (unlikely(element == NULL)) {
+					count_vm_event(PGPOOL_REFILL_FAILED);
+					msleep(10);
+					continue;
+				}
+
+				mempool_free(element, pool);
+			}
+		}
+
+		refill = 0;
+	}
+
+	return 0;
+}
+
+static int stat_show(struct seq_file *s, void *data)
+{
+	mempool_t *pool = NULL;
+	int mtype;
+	int order;
+
+	for_each_pmempool(pool, mtype, order) {
+		if (pool == NULL)
+			continue;
+
+		seq_printf(s, "%s order %d\n", migratetype_names[mtype], order);
+		seq_printf(s, "  min_nr   %d\n", pool->min_nr);
+		seq_printf(s, "  curr_nr  %d\n", pool->curr_nr);
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(stat);
+
+static int enabled_show(struct seq_file *s, void *data)
+{
+	seq_printf(s, "%d\n", enabled);
+
+	return 0;
+}
+
+static ssize_t enabled_write(struct file *f, const char __user *ubuf, size_t len,
+			     loff_t *offset)
+{
+	if (kstrtobool_from_user(ubuf, len, &enabled))
+		return -EINVAL;
+
+	return len;
+}
+DEFINE_SHOW_STORE_ATTRIBUTE(enabled);
+
+static int __init priority_mempool_init(void)
+{
+	mempool_t *pool = NULL;
+	int mtype;
+	int order;
+
+	for_each_pmempool(pool, mtype, order) {
+		pmempool[mtype][order] = mempool_create_node(MEMPOOL_MIN_NR,
+							mempool_alloc_pages,
+							mempool_free_pages,
+							(void *)(long)order,
+							migratetype_gfp(mtype),
+							NUMA_NO_NODE);
+	}
+
+	init_waitqueue_head(&wq);
+	kpmempoold = kthread_run(pmempool_refill, NULL, "kpmempoold");
+	if (IS_ERR(kpmempoold)) {
+		pr_err("Failed to start kpmempoold，ret=%ld\n",
+					PTR_ERR(kpmempoold));
+	}
+
+	if (debugfs_initialized()) {
+		pmempool_debugfs_dir = debugfs_create_dir("pmempool", NULL);
+		debugfs_create_file("stat", 0444, pmempool_debugfs_dir, NULL,
+				    &stat_fops);
+		debugfs_create_file("enabled", 0644, pmempool_debugfs_dir, NULL,
+				    &enabled_fops);
+	}
+
+	return 0;
+}
+
+static void __exit priority_mempool_exit(void)
+{
+	mempool_t *pool;
+	int mtype;
+	int order;
+
+	debugfs_remove(pmempool_debugfs_dir);
+	kthread_stop(kpmempoold);
+
+	for_each_pmempool(pool, mtype, order)
+		mempool_destroy(pool);
+}
+
+module_init(priority_mempool_init);
+module_exit(priority_mempool_exit);
+
+MODULE_AUTHOR("Vernon Yang <vernon2gm@gmail.com>");
+MODULE_DESCRIPTION("PRIORITY MEMPOOL MODULE");
+MODULE_LICENSE("GPL");
diff --git a/mm/vmscan.c b/mm/vmscan.c
index c767d71c43d7..95234bf9f883 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -445,10 +445,14 @@ static int reclaimer_offset(void)
 {
 	BUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=
 			PGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);
+	BUILD_BUG_ON(PGSTEAL_DIRECT_KWORKER - PGSTEAL_KSWAPD !=
+			PGDEMOTE_DIRECT_KWORKER - PGDEMOTE_KSWAPD);
 	BUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=
 			PGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);
 	BUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=
 			PGSCAN_DIRECT - PGSCAN_KSWAPD);
+	BUILD_BUG_ON(PGSTEAL_DIRECT_KWORKER - PGSTEAL_KSWAPD !=
+			PGSCAN_DIRECT_KWORKER - PGSCAN_KSWAPD);
 	BUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=
 			PGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);
 
@@ -456,6 +460,8 @@ static int reclaimer_offset(void)
 		return 0;
 	if (current_is_khugepaged())
 		return PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;
+	if (current_is_direct_kworker())
+		return PGSTEAL_DIRECT_KWORKER - PGSTEAL_KSWAPD;
 	return PGSTEAL_DIRECT - PGSTEAL_KSWAPD;
 }
 
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 16bfe1c694dd..1899922a30db 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -1272,6 +1272,7 @@ const char * const vmstat_text[] = {
 #endif
 	"pgdemote_kswapd",
 	"pgdemote_direct",
+	"pgdemote_direct_kworker",
 	"pgdemote_khugepaged",
 #ifdef CONFIG_HUGETLB_PAGE
 	"nr_hugetlb",
@@ -1306,9 +1307,11 @@ const char * const vmstat_text[] = {
 	"pgreuse",
 	"pgsteal_kswapd",
 	"pgsteal_direct",
+	"pgsteal_direct_kworker",
 	"pgsteal_khugepaged",
 	"pgscan_kswapd",
 	"pgscan_direct",
+	"pgscan_direct_kworker",
 	"pgscan_khugepaged",
 	"pgscan_direct_throttle",
 	"pgscan_anon",
@@ -1316,6 +1319,13 @@ const char * const vmstat_text[] = {
 	"pgsteal_anon",
 	"pgsteal_file",
 
+#ifdef CONFIG_PMEMPOOL
+	"pgpool_success",
+	"pgpool_failed",
+	"pgpool_refill_failed",
+	"pgpool_refill_wakeup",
+#endif
+
 #ifdef CONFIG_NUMA
 	"zone_reclaim_success",
 	"zone_reclaim_failed",
-- 
2.34.1

