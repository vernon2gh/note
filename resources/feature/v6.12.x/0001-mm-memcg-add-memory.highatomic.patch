From 9806c30084da722265232cff3d20b23e92a0ed67 Mon Sep 17 00:00:00 2001
From: Vernon Yang <vernon2gm@gmail.com>
Date: Wed, 5 Mar 2025 20:45:31 +0800
Subject: [PATCH] mm: memcg: add memory.highatomic

By adding the memory.highatomic node to the cgroup, when setting it, the
user space process in this cgroup can prioritize memory allocation, which
is equivalent to using highatomic memory pool to reduce the number of
times the execution of direct memory reclaim.

Signed-off-by: Vernon Yang <vernon2gm@gmail.com>
---
 include/linux/memcontrol.h    |  3 ++
 include/linux/mmzone.h        | 20 ++++++++++
 include/linux/vm_event_item.h |  1 +
 mm/memcontrol.c               | 36 ++++++++++++++++++
 mm/mm_init.c                  |  7 +++-
 mm/page_alloc.c               | 71 +++++++++++++++++++++++++++++++++--
 mm/vmstat.c                   |  9 +++++
 7 files changed, 142 insertions(+), 5 deletions(-)

diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index e1b41554a5fb..14f552f12d42 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -220,6 +220,9 @@ struct mem_cgroup {
 	 */
 	bool oom_group;
 
+	/* Allows access to MIGRATE_HIGHATOMIC */
+	bool highatomic;
+
 	int swappiness;
 
 	/* memory.events and memory.events.local */
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 80bc5640bb60..6124a9f24458 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1495,6 +1495,26 @@ static inline int local_memory_node(int node_id) { return node_id; };
  */
 #define zone_idx(zone)		((zone) - (zone)->zone_pgdat->node_zones)
 
+static inline unsigned int nr_free_highatomic(struct zone *zone, int limit_rate)
+{
+	int nr;
+
+	nr = zone_managed_pages(zone) / limit_rate;
+
+#ifdef CONFIG_ZONE_DMA
+	if (zone_idx(zone) == ZONE_DMA)
+		nr = 0;
+#endif
+
+	if (nr)
+		nr = ALIGN(nr, pageblock_nr_pages);
+
+	return nr;
+}
+
+#define nr_free_highatomic_low(zone)	nr_free_highatomic(zone, 1000)
+#define nr_free_highatomic_high(zone)	nr_free_highatomic(zone, 250)
+
 #ifdef CONFIG_ZONE_DEVICE
 static inline bool zone_is_zone_device(struct zone *zone)
 {
diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h
index f70d0958095c..f131a8e8c9bd 100644
--- a/include/linux/vm_event_item.h
+++ b/include/linux/vm_event_item.h
@@ -182,6 +182,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		KSTACK_REST,
 #endif
 #endif /* CONFIG_DEBUG_STACK_USAGE */
+		HIGHATOMIC_HIT,
 		NR_VM_EVENT_ITEMS
 };
 
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 53db98d2c4a1..679edec7f50d 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -451,6 +451,7 @@ static const unsigned int memcg_vm_event_stat[] = {
 	NUMA_PTE_UPDATES,
 	NUMA_HINT_FAULTS,
 #endif
+	HIGHATOMIC_HIT,
 };
 
 #define NR_MEMCG_EVENTS ARRAY_SIZE(memcg_vm_event_stat)
@@ -4270,6 +4271,35 @@ static ssize_t memory_oom_group_write(struct kernfs_open_file *of,
 	return nbytes;
 }
 
+static int memory_highatomic_show(struct seq_file *m, void *v)
+{
+        struct mem_cgroup *memcg = mem_cgroup_from_seq(m);
+
+        seq_printf(m, "%d\n", READ_ONCE(memcg->highatomic));
+
+        return 0;
+}
+
+static ssize_t memory_highatomic_write(struct kernfs_open_file *of,
+                                     char *buf, size_t nbytes, loff_t off)
+{
+        struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
+        bool highatomic;
+        int ret;
+
+        buf = strstrip(buf);
+        if (!buf)
+                return -EINVAL;
+
+        ret = kstrtobool(buf, &highatomic);
+        if (ret)
+                return ret;
+
+        WRITE_ONCE(memcg->highatomic, highatomic);
+
+        return nbytes;
+}
+
 enum {
 	MEMORY_RECLAIM_SWAPPINESS = 0,
 	MEMORY_RECLAIM_NULL,
@@ -4412,6 +4442,12 @@ static struct cftype memory_files[] = {
 		.seq_show = memory_oom_group_show,
 		.write = memory_oom_group_write,
 	},
+        {
+                .name = "highatomic",
+                .flags = CFTYPE_NOT_ON_ROOT | CFTYPE_NS_DELEGATABLE,
+                .seq_show = memory_highatomic_show,
+                .write = memory_highatomic_write,
+        },
 	{
 		.name = "reclaim",
 		.flags = CFTYPE_NS_DELEGATABLE,
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 4ba5607aaf19..1e7b83da0787 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -921,6 +921,7 @@ static void __init memmap_init_zone_range(struct zone *zone,
 	unsigned long zone_start_pfn = zone->zone_start_pfn;
 	unsigned long zone_end_pfn = zone_start_pfn + zone->spanned_pages;
 	int nid = zone_to_nid(zone), zone_id = zone_idx(zone);
+	int reserve_highatomic_nr = nr_free_highatomic_low(zone);
 
 	start_pfn = clamp(start_pfn, zone_start_pfn, zone_end_pfn);
 	end_pfn = clamp(end_pfn, zone_start_pfn, zone_end_pfn);
@@ -928,7 +929,11 @@ static void __init memmap_init_zone_range(struct zone *zone,
 	if (start_pfn >= end_pfn)
 		return;
 
-	memmap_init_range(end_pfn - start_pfn, nid, zone_id, start_pfn,
+	zone->nr_reserved_highatomic += reserve_highatomic_nr;
+	memmap_init_range(reserve_highatomic_nr, nid, zone_id, start_pfn,
+			  zone_end_pfn, MEMINIT_EARLY, NULL, MIGRATE_HIGHATOMIC);
+	memmap_init_range(end_pfn - start_pfn - reserve_highatomic_nr, nid, zone_id,
+			  start_pfn + reserve_highatomic_nr,
 			  zone_end_pfn, MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);
 
 	if (*hole_pfn < start_pfn)
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index b6958333054d..1b13b38d568b 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1602,6 +1602,13 @@ struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
 		trace_mm_page_alloc_zone_locked(page, order, migratetype,
 				pcp_allowed_order(order) &&
 				migratetype < MIGRATE_PCPTYPES);
+
+		if (migratetype == MIGRATE_HIGHATOMIC) {
+			__count_vm_events(HIGHATOMIC_HIT, 1 << order);
+			__count_memcg_events(mem_cgroup_from_task(current),
+						HIGHATOMIC_HIT, 1 << order);
+		}
+
 		return page;
 	}
 
@@ -2102,6 +2109,9 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 					pageblock_nr_pages)
 			continue;
 
+		if (READ_ONCE(zone->nr_free_highatomic) < nr_free_highatomic_high(zone))
+			continue;
+
 		spin_lock_irqsave(&zone->lock, flags);
 		for (order = 0; order < NR_PAGE_ORDERS; order++) {
 			struct free_area *area = &(zone->free_area[order]);
@@ -2130,6 +2140,10 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 				 */
 				size = max(pageblock_nr_pages, 1UL << order);
 				size = min(size, zone->nr_reserved_highatomic);
+
+				if ((READ_ONCE(zone->nr_free_highatomic) - size) < nr_free_highatomic_high(zone))
+					break;
+
 				zone->nr_reserved_highatomic -= size;
 			}
 
@@ -2638,6 +2652,39 @@ static void free_unref_page_commit(struct zone *zone, struct per_cpu_pages *pcp,
 	}
 }
 
+static inline bool foreground_highatomic_low(struct zone *zone)
+{
+	// direct memory reclaim
+	if (((current->flags & PF_MEMALLOC) && !current_is_kswapd()))
+		return false;
+
+	if (READ_ONCE(zone->nr_free_highatomic) >= nr_free_highatomic_low(zone))
+		return false;
+
+	return true;
+}
+
+static inline bool foreground_highatomic_fill(struct zone *zone)
+{
+	struct mem_cgroup *memcg;
+
+	if (mem_cgroup_disabled())
+		return true;
+
+	rcu_read_lock();
+	memcg = mem_cgroup_from_task(current);
+	if (!memcg || !READ_ONCE(memcg->highatomic)) {
+		rcu_read_unlock();
+		return true;
+	}
+	rcu_read_unlock();
+
+	if (READ_ONCE(zone->nr_free_highatomic) >= nr_free_highatomic_high(zone))
+		return false;
+
+	return true;
+}
+
 /*
  * Free a pcp page
  */
@@ -2645,7 +2692,7 @@ void free_unref_page(struct page *page, unsigned int order)
 {
 	unsigned long __maybe_unused UP_flags;
 	struct per_cpu_pages *pcp;
-	struct zone *zone;
+	struct zone *zone = page_zone(page);
 	unsigned long pfn = page_to_pfn(page);
 	int migratetype;
 
@@ -2665,15 +2712,20 @@ void free_unref_page(struct page *page, unsigned int order)
 	 * excessively into the page allocator
 	 */
 	migratetype = get_pfnblock_migratetype(page, pfn);
+
+	if (migratetype == MIGRATE_HIGHATOMIC && foreground_highatomic_low(zone)) {
+		free_one_page(zone, page, pfn, order, FPI_NONE);
+		return;
+	}
+
 	if (unlikely(migratetype >= MIGRATE_PCPTYPES)) {
 		if (unlikely(is_migrate_isolate(migratetype))) {
-			free_one_page(page_zone(page), page, pfn, order, FPI_NONE);
+			free_one_page(zone, page, pfn, order, FPI_NONE);
 			return;
 		}
 		migratetype = MIGRATE_MOVABLE;
 	}
 
-	zone = page_zone(page);
 	pcp_trylock_prepare(UP_flags);
 	pcp = pcp_spin_trylock(zone->per_cpu_pageset);
 	if (pcp) {
@@ -3477,7 +3529,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 			 * If this is a high-order atomic allocation then check
 			 * if the pageblock should be reserved for the future
 			 */
-			if (unlikely(alloc_flags & ALLOC_HIGHATOMIC))
+			if (unlikely((alloc_flags & ALLOC_HIGHATOMIC) &&
+				      foreground_highatomic_fill(zone)))
 				reserve_highatomic_pageblock(page, order, zone);
 
 			return page;
@@ -4033,6 +4086,16 @@ gfp_to_alloc_flags(gfp_t gfp_mask, unsigned int order)
 	} else if (unlikely(rt_or_dl_task(current)) && in_task())
 		alloc_flags |= ALLOC_MIN_RESERVE;
 
+	if (!mem_cgroup_disabled()) {
+		struct mem_cgroup *memcg;
+
+		rcu_read_lock();
+		memcg = mem_cgroup_from_task(current);
+		if (memcg && READ_ONCE(memcg->highatomic))
+			alloc_flags |= ALLOC_HIGHATOMIC;
+		rcu_read_unlock();
+	}
+
 	alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, alloc_flags);
 
 	return alloc_flags;
diff --git a/mm/vmstat.c b/mm/vmstat.c
index ac6a5aa34eab..7a684285c14b 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -1463,6 +1463,7 @@ const char * const vmstat_text[] = {
 	"kstack_rest",
 #endif
 #endif
+	"highatomic_hit",
 #endif /* CONFIG_VM_EVENT_COUNTERS || CONFIG_MEMCG */
 };
 #endif /* CONFIG_PROC_FS || CONFIG_SYSFS || CONFIG_NUMA || CONFIG_MEMCG */
@@ -1750,6 +1751,10 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   "\n        spanned  %lu"
 		   "\n        present  %lu"
 		   "\n        managed  %lu"
+		   "\n        reserved_highatomic %lu"
+		   "\n        free_highatomic     %lu"
+		   "\n        free_highatomic_low  %u"
+		   "\n        free_highatomic_high %u"
 		   "\n        cma      %lu",
 		   zone_page_state(zone, NR_FREE_PAGES),
 		   zone->watermark_boost,
@@ -1760,6 +1765,10 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 		   zone->spanned_pages,
 		   zone->present_pages,
 		   zone_managed_pages(zone),
+		   zone->nr_reserved_highatomic,
+		   READ_ONCE(zone->nr_free_highatomic),
+		   nr_free_highatomic_low(zone),
+		   nr_free_highatomic_high(zone),
 		   zone_cma_pages(zone));
 
 	seq_printf(m,
-- 
2.34.1

